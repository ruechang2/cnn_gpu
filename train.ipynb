{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習方法の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#!pip install scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import sys, os\n",
    "#import torch\n",
    "sys.path.append(os.pardir) \n",
    "from collections import OrderedDict\n",
    "from common.layers import Convolution, MaxPooling, ReLU, Affine, SoftmaxWithLoss, SGD, RMSProp, Adam, Adadelta, Dropout, MeanSquaredLoss, LeakyReLU, mean_squared_error, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kota/Downloads/dl_online_2022_1_submit_katakana_kota_isayama_20220215_1\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#! pip install torch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#!conda install torch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#!conda install -c pytorch pytorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      6\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#! pip install torch\n",
    "#!conda install torch\n",
    "#!conda install -c pytorch pytorch\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else :\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 21:28:31.228084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 21:28:31.783329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-23 21:28:32.350928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-23 21:28:32.394967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-23 21:28:32.395139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-23 21:28:32.809825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-23 21:28:32.809998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-23 21:28:32.810127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-23 21:28:32.810234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 21593 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3258023619576457061\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 22642753536\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 13344350284389445685\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを読む1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48521, 28, 28)\n",
      "(48521, 15)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(\"/home/kota/Downloads/dl_online_2022_1_submit_katakana_kota_isayama_20220215_1/1_data/new/train_data.npy\")\n",
    "#train_data = np.reshape(train_data, (50970, 1, 28, 28))\n",
    "train_label = np.load(\"/home/kota/Downloads/dl_online_2022_1_submit_katakana_kota_isayama_20220215_1/1_data/new/train_label.npy\")\n",
    "#train_label = np.reshape(train_label, (9000, 2352))\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 正規化\n",
    "train_data =  (train_data - train_data.min()) / train_data.max()\n",
    "#train_data = (train_data)/255\n",
    "train_data = train_data.astype('float32')\n",
    "#print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape= (48521, 784)\n"
     ]
    }
   ],
   "source": [
    "# 配列形式変更\n",
    "train_data = train_data.reshape(-1, 28*28)\n",
    "#train_label = train_label.reshape(-1, 28*28)\n",
    "print(\"train_data.shape=\", train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainとtestに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33964, 784) (14557, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, \n",
    "                                                                                        test_size=0.3, random_state=1234,\n",
    "                                                                                        shuffle=True\n",
    "                                                                                       )\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "\n",
    "train = X_train\n",
    "test = X_test\n",
    "#train = train.reshape(-1, 28*28)\n",
    "#test = test.reshape(-1, 28*28)\n",
    "#train_labels = lb.fit_transform(y_train)\n",
    "#test_labels = lb.fit_transform(y_test)\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "\n",
    "#print(train_labels.shape)\n",
    "#print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reshape(-1, 1, 28, 28)\n",
    "test = test.reshape(-1, 1, 28, 28)\n",
    "#train_labels = train_labels.reshape(-1, 1, 28, 28)\n",
    "#test_labels = test_labels.reshape(-1, 1, 28, 28)\n",
    "#print(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        \n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n",
    "       \n",
    "\n",
    "\n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "        # レイヤの生成===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(MaxPooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(MaxPooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(MaxPooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Dropout(0.15))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        #self.layers.append(Dropout(0.25))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33964, 15)\n",
      "epoch=0\n",
      "it= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21475/2597010849.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  iter_num = np.ceil(xsize / batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it= 1\n",
      "it= 2\n",
      "it= 3\n",
      "it= 4\n",
      "it= 5\n",
      "it= 6\n",
      "it= 7\n",
      "it= 8\n",
      "it= 9\n",
      "it= 10\n",
      "it= 11\n",
      "it= 12\n",
      "it= 13\n",
      "it= 14\n",
      "it= 15\n",
      "it= 16\n",
      "it= 17\n",
      "it= 18\n",
      "it= 19\n",
      "it= 20\n",
      "it= 21\n",
      "it= 22\n",
      "it= 23\n",
      "it= 24\n",
      "it= 25\n",
      "it= 26\n",
      "it= 27\n",
      "it= 28\n",
      "it= 29\n",
      "it= 30\n",
      "it= 31\n",
      "it= 32\n",
      "it= 33\n",
      "it= 34\n",
      "it= 35\n",
      "it= 36\n",
      "it= 37\n",
      "it= 38\n",
      "it= 39\n",
      "it= 40\n",
      "it= 41\n",
      "it= 42\n",
      "it= 43\n",
      "it= 44\n",
      "it= 45\n",
      "it= 46\n",
      "it= 47\n",
      "it= 48\n",
      "it= 49\n",
      "it= 50\n",
      "it= 51\n",
      "it= 52\n",
      "it= 53\n",
      "it= 54\n",
      "it= 55\n",
      "it= 56\n",
      "it= 57\n",
      "it= 58\n",
      "it= 59\n",
      "it= 60\n",
      "it= 61\n",
      "it= 62\n",
      "it= 63\n",
      "it= 64\n",
      "it= 65\n",
      "it= 66\n",
      "it= 67\n",
      "it= 68\n",
      "it= 69\n",
      "it= 70\n",
      "it= 71\n",
      "it= 72\n",
      "it= 73\n",
      "it= 74\n",
      "it= 75\n",
      "it= 76\n",
      "it= 77\n",
      "it= 78\n",
      "it= 79\n",
      "it= 80\n",
      "it= 81\n",
      "it= 82\n",
      "it= 83\n",
      "it= 84\n",
      "it= 85\n",
      "it= 86\n",
      "it= 87\n",
      "it= 88\n",
      "it= 89\n",
      "it= 90\n",
      "it= 91\n",
      "it= 92\n",
      "it= 93\n",
      "it= 94\n",
      "it= 95\n",
      "it= 96\n",
      "it= 97\n",
      "it= 98\n",
      "it= 99\n",
      "it= 100\n",
      "it= 101\n",
      "it= 102\n",
      "it= 103\n",
      "it= 104\n",
      "it= 105\n",
      "it= 106\n",
      "it= 107\n",
      "it= 108\n",
      "it= 109\n",
      "it= 110\n",
      "it= 111\n",
      "it= 112\n",
      "it= 113\n",
      "it= 114\n",
      "it= 115\n",
      "it= 116\n",
      "it= 117\n",
      "it= 118\n",
      "it= 119\n",
      "it= 120\n",
      "it= 121\n",
      "it= 122\n",
      "it= 123\n",
      "it= 124\n",
      "it= 125\n",
      "it= 126\n",
      "it= 127\n",
      "it= 128\n",
      "it= 129\n",
      "it= 130\n",
      "it= 131\n",
      "it= 132\n"
     ]
    }
   ],
   "source": [
    "x = train\n",
    "t = train_labels\n",
    "#t = np.reshape(t, (500, 15))\n",
    "print(t.shape)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "lr = 0.01\n",
    "rho = 0.9\n",
    "#optimizer = SGD(lr=lr)\n",
    "#optimizer = RMSProp(lr=0.0015, rho=0.9)\n",
    "#optimizer = Adam(lr = 0.0015)\n",
    "optimizer = Adadelta(rho = 0.9)\n",
    "x = x.reshape(-1,1,28,28)\n",
    "#t = t.reshape(-1, 1, 28, 28)\n",
    "#print(t.shape)\n",
    "\n",
    "#print(x.shape)\n",
    "\n",
    "# 繰り返し回数\n",
    "xsize = x.shape[0]\n",
    "#print(xsize)\n",
    "iter_num = np.ceil(xsize / batch_size).astype(np.int)\n",
    "\n",
    "# 2層NNのオブジェクト生成\n",
    "#tnet = TwoLayerNet(input_size=28*28, hidden_size=10, output_size=15)\n",
    "dnet = DeepConvNet(input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=15)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#print(\"初期パラメータ\")\n",
    "#print(snet.params[\"W1\"].round(4))\n",
    "#print(snet.params[\"b1\"].round(4))\n",
    "#print(snet.params[\"W2\"].round(4))\n",
    "#print(snet.params[\"b2\"].round(4))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch=%s\"%epoch)\n",
    "    \n",
    "    # シャッフル\n",
    "    idx = np.arange(xsize)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    for it in range(iter_num):\n",
    "        \"\"\"\n",
    "        ランダムなミニバッチを順番に取り出す\n",
    "        \"\"\"\n",
    "        print(\"it=\", it)\n",
    "        mask = idx[batch_size*it : batch_size*(it+1)]\n",
    "    \n",
    "        # ミニバッチの生成\n",
    "        x_train = x[mask]\n",
    "        t_train = t[mask]\n",
    "        #print(x_train.shape, t_train.shape)\n",
    "        # 勾配の計算\n",
    "        grads = dnet.gradient(x_train, t_train)\n",
    "        #print(grads)\n",
    "        #print(snet.params.shape)\n",
    "        #print(lr)\n",
    "        optimizer.update(dnet.params, grads)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## 学習経過の記録\n",
    "    \n",
    "    # 訓練データにおけるloss\n",
    "    train_loss.append(dnet.loss(x,  t))\n",
    "    \n",
    "    # テストデータにおけるloss\n",
    "    test_loss.append(dnet.loss(test, test_labels))\n",
    "    \n",
    "    # 訓練データにて精度を確認\n",
    "    train_accuracy.append(dnet.accuracy(x, t))\n",
    "\n",
    "    # テストデータにて精度を算出\n",
    "    test_accuracy.append(dnet.accuracy(test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lossとaccuracyのグラフ化\n",
    "df_log = pd.DataFrame({\"train_loss\":train_loss,\n",
    "             \"test_loss\":test_loss,\n",
    "             \"train_accuracy\":train_accuracy,\n",
    "             \"test_accuracy\":test_accuracy})\n",
    "\n",
    "df_log.plot(style=['r-', 'r--', 'b-', 'b--'])\n",
    "plt.ylim([0,7])\n",
    "plt.ylabel(\"Accuracy or loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"katakana_model.pickle\", \"wb\") as f:\n",
    "    pickle.dump(dnet, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
